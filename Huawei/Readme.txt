-----------------------------------------------2019.3.29----------------------------------------------------------------
预处理：                1. CSI300数据下载  →  2. 筛选（数据缺失严重）  →  3. 规范化  →  4. 聚类  →  5. 选类别代表（质心）
                                ✔                       ✔                       ✔            ✔                 ✔
                                                                                                                 丨
                                                                                                                 ↓
第一步（爬数据）：                                                                                       6. 按具体股吧爬数据
                                                                                                                 ✔
                                                                                                                 丨
                                                                                                                 ↓
第二步（情感分析）：    10. 规范化时间长度  ←  9. CNN得出情感值[1,-1] ✔ ←  8. 句向量（doc2vc）✔  ←  7.分词(✔)

                               丨
                               ↓
第三步（股票预测）：    11. 建模预测股票（数据/自编码特征+LSTM）


-----------------------------------------------2019.4.2 wangren 更新----------------------------------------------------
文件：
"abandon_"  ：   弃用的文件
"hs300_stocks.csv"  :   沪深300股票信息
"loss_count.csv"  ：  300支股票每个属性缺失的个数。 行：股票， 列：各属性缺失的个数
"loss_data.csv"  ：  筛选掉的股票代码存档
"filtered_data.csv"  ：  筛选后的股票数据（date,code,open,close,high,low）



文件夹：
"datas"  :    沪深300，2010至今（2019-04-01）的数据（date,code,open,close,high,low,volume,amount,turn）


Python:
"downloadData.py"  :    1.get_code()：获取沪深300股票信息并保存
                        2.get_data(): 获取300支股票的数据
                        3.get_count(): 股票代码 + 记录股票每个属性的缺失数量 + 股票总数，结果以行追加在csv的中
                        4.filter(): 筛选后的数据，并合并为一个csv文件
-----------------------------------------------2019.4.3 更新----------------------------------------------------
文件：
"center.csv"             ： 质心股票的信息
"cluster num.csv"        ：每一类股票信息分开存放


文件夹：
"CLuster":    聚类后的股票代码信息


Python:
"Complete-clustering.py:1.preprocess_data()：规整数据并规范化
                        2.cluster(): AP近邻算法聚类
                        3.visual_stock_relationship(): 画图

 -----------------------------------------------2019.5.18 更新----------------------------------------------------
数据文件data文件夹：

all_news.csv:中国太保训练出的句向量文件
dfcw.csv:中国太保爬虫文件
news.csv:中国太保单评论文件
sh.601601_data.csv:中国太保价格文件

news_yuliao:以中国平安作为训练句向量的语料库
yuliaoku.csv:中国平安单评论文件

sh.601601_train_data.csv:自编码器部分的训练集（中国太保）

newsVec_train.csv:中国人寿爬虫文件（作为CNN部分的训练集）
sh.601628_train_data.csv:中国人寿价格
newsVec_train_process.csv：中国人寿处理过的文件（去掉符号）
all_news_train.csv:CNN最终训练集（向量文件）

-----------------------------------------------2019.5.20 更新----------------------------------------------------

文件夹models：
ko_d2v.d2v：训练句向量模型
keras_selfcoding_trained_model.h5：自编码器模型
keras_CNN_trained_model.h5：卷积模型

爬虫模块：
clawer.py:爬取所需股票的评论信息，包括阅读量，回复数量，评论标题，作者，发布时间
downloadData.py：获取沪深300股票价格信息，并进行筛选
getprice.py：获取股票信息并写入csv文件

数据分析模块：
Complete-clustering.py：对筛选后的股票价格信息进行聚类，得到相似股票类
textprocess.py：对爬虫获取的含评论文件进行处理，如去掉特殊字符，并提取出评论存入csv文件
Doc2vec.py：对评论文件进行分词并训练为句向量文件
newsVec.py：利用CNN卷积神经网络训练文件特征
selfcoding.py：利用CNN建立自编码器，输入价格提取价格特征

数据应用模块：